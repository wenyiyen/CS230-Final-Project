{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essentials\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import keras\n",
    "from keras.models import Sequential,load_model,model_from_json,Model\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.layers import Dense,Conv2D,Activation,Flatten,Dropout,GlobalAveragePooling2D,Lambda\n",
    "from keras.engine.input_layer import Input\n",
    "from keras.applications import *\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import *\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import keras.backend as K\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import csv\n",
    "import h5py\n",
    "\n",
    "# trial series to track our iterations\n",
    "trial_num = 20\n",
    "data_change = True # if the split of data is changed or images are not generated yet\n",
    "extract_feature = True # if the model features haven't been extracted\n",
    "\n",
    "# data info and organizations\n",
    "img_width,img_height = 224,224\n",
    "batch_size = 8\n",
    "epochs = 50\n",
    "# no / partial / collapse: total 515 {0: 240, 1: 223, 2: 52}\n",
    "nb_class_train = np.array([240, 220, 52]) # do validation split later in code\n",
    "over_sample_train = np.array([1, 1, 1]) # make sure %batch_size = 0\n",
    "nb_train_samples = np.dot(nb_class_train, over_sample_train.T)\n",
    "print(nb_train_samples)\n",
    "nb_test_samples = 129\n",
    "num_classes = nb_class_train.shape[0]\n",
    "channel = 3\n",
    "\n",
    "# models used\n",
    "models = {'ResNet50': [ResNet50, (224, 224), resnet50.preprocess_input], \\\n",
    "          'InceptionV3': [InceptionV3, (299, 299), inception_v3.preprocess_input], \\\n",
    "          'Xception': [Xception, (299, 299), xception.preprocess_input], \\\n",
    "          'VGG16': [VGG16, (224, 224), vgg16.preprocess_input], \\\n",
    "          'VGG19': [VGG19, (224, 224), vgg19.preprocess_input], \\\n",
    "          'InceptionResNetV2': [InceptionResNetV2, (299, 299), inception_resnet_v2.preprocess_input], \\\n",
    "          'MobileNet': [MobileNet, (224, 224), mobilenet.preprocess_input], \\\n",
    "          'MobileNetV2': [MobileNetV2, (224, 224), mobilenet.preprocess_input], \\\n",
    "#           'DenseNet121': [DenseNet121, (224, 224), densenet.preprocess_input], \\\n",
    "#           'DenseNet169': [DenseNet169, (224, 224), densenet.preprocess_input], \\\n",
    "#           'DenseNet201': [DenseNet201, (224, 224), densenet.preprocess_input], \\\n",
    "          'NASNetMobile': [NASNetMobile, (224, 224), nasnet.preprocess_input], \\\n",
    "          'NASNetLarge': [NASNetLarge, (331, 331), nasnet.preprocess_input]\n",
    "         }\n",
    "# models = {'VGG16': [VGG16, (299, 299), vgg16.preprocess_input]}\n",
    "\n",
    "# path info\n",
    "org_X_train_data_dir = './data_org/X_train.npy'\n",
    "org_Y_train_data_dir = './data_org/Y_train.npy'\n",
    "train_data_dir = './data_models/train'\n",
    "test_data_dir = './data_models/test'\n",
    "\n",
    "feature_path = './model_features'\n",
    "\n",
    "best_model_weights_path = './merge_results/best_weights' + str(trial_num) + '.h5'\n",
    "final_model_weights_path = './merge_results/final_weights' + str(trial_num) + '.h5'\n",
    "model_save_path = './merge_results/model' + str(trial_num) + '.json'\n",
    "learning_hist_path = './merge_results/learning_hist' + str(trial_num) + '.npy'\n",
    "acc_plt_path1 = './merge_results/acc_plt1_' + str(trial_num) + '.png'\n",
    "acc_plt_path2 = './merge_results/acc_plt2_' + str(trial_num) + '.png'\n",
    "loss_plt_path1 = './merge_results/loss_plt1_' + str(trial_num) + '.png'\n",
    "loss_plt_path2 = './merge_results/loss_plt2_' + str(trial_num) + '.png'\n",
    "Y_predict_path = './merge_results/Y_predict' + str(trial_num) + '.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate temporary images for Keras' sequence\n",
    "def process_data():\n",
    "    # load data\n",
    "    X_train = np.load(org_X_train_data_dir);\n",
    "    Y_train = np.load(org_Y_train_data_dir);\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        # make directory\n",
    "        os.makedirs(train_data_dir+'/'+str(c))\n",
    "        # over sampling training data\n",
    "        index_c = np.where(Y_train == c)[0]\n",
    "        X_train_c = X_train[index_c[0:nb_class_train[c]],:,:,:]\n",
    "        X_train_new_c = np.tile(X_train_c,(int(over_sample_train[c]),1,1,1))\n",
    "        extra_c = int((over_sample_train[c] - int(over_sample_train[c])) * nb_class_train[c])\n",
    "        X_train_new_c = np.concatenate((X_train_new_c, X_train_c[0:extra_c,:,:,:]))\n",
    "        # generate images\n",
    "        for i in range(X_train_new_c.shape[0]):\n",
    "            im = Image.fromarray(X_train_new_c[i, :, :, :])\n",
    "            im.save(train_data_dir+'/'+str(c)+'/'+str(c)+'_'+str(i)+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-generate images if data split is changed\n",
    "if data_change:\n",
    "    # delete all files and folders in directory\n",
    "    shutil.rmtree(train_data_dir)\n",
    "    # save new images\n",
    "    process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function extracts the bottleneck features from a specified model\n",
    "def write_gap(MODEL, model_str, image_size, lambda_func):\n",
    "    width = image_size[0]\n",
    "    height = image_size[1]\n",
    "    input_tensor = Input((height, width, 3))\n",
    "    x = input_tensor\n",
    "    if lambda_func:\n",
    "        x = Lambda(lambda_func)(x)\n",
    "    \n",
    "    # attach input and output layers to the pretrained model\n",
    "    base_model = MODEL(input_tensor=x, weights='imagenet', include_top=False)\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "    \n",
    "    # generate data and predict\n",
    "#     gen1 = ImageDataGenerator(rescale=1./255, # change here\n",
    "#                                 shear_range=0.2,\n",
    "#                                 zoom_range=0.2,\n",
    "#                                 horizontal_flip=True)\n",
    "    gen1 = ImageDataGenerator()\n",
    "    gen2 = ImageDataGenerator()\n",
    "    train_generator = gen1.flow_from_directory(train_data_dir, image_size, shuffle=False, batch_size=batch_size)\n",
    "    test_generator = gen2.flow_from_directory(test_data_dir, image_size, shuffle=False, batch_size=1, class_mode=None)\n",
    "    train = model.predict_generator(train_generator, train_generator.samples // batch_size)\n",
    "    test = model.predict_generator(test_generator, test_generator.samples)\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    print(train_generator.classes.shape)\n",
    "#     print base_model.summary()\n",
    "    # save features to file\n",
    "    with h5py.File(feature_path+'/gap_%s.h5'%model_str) as h:\n",
    "        h.create_dataset(\"train\", data=train)\n",
    "        h.create_dataset(\"test\", data=test)\n",
    "        h.create_dataset(\"label\", data=train_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract bottle neck feature from several pre-trained models\n",
    "if extract_feature:\n",
    "    # delete files in the directory\n",
    "    shutil.rmtree(feature_path)\n",
    "    os.makedirs(feature_path)\n",
    "    # extract features\n",
    "    for key, value in models.iteritems():\n",
    "        write_gap(value[0], key, value[1], value[2])\n",
    "        print \"finish \" + key\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute F1 score as a custom metric to return\n",
    "def F1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    \n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read features of different models and shuffle\n",
    "hist_info = {}\n",
    "y_preds = {}\n",
    "# loop through different models and fit model\n",
    "for key, value in models.iteritems():\n",
    "    filename = feature_path+'/gap_%s.h5'%key\n",
    "    with h5py.File(filename, 'r') as h:\n",
    "        X_train = np.array(h['train'])\n",
    "        X_test = np.array(h['test'])\n",
    "        y_train = np.array(h['label'])\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "\n",
    "    # shuffle\n",
    "    np.random.seed(2018)\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    \n",
    "    # build and train model\n",
    "    y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
    "\n",
    "    input_tensor = Input(X_train.shape[1:])\n",
    "    print(X_train.shape[1:])\n",
    "    x = Dropout(0.5)(input_tensor)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(input_tensor, x)\n",
    "    print model.summary()\n",
    "\n",
    "    sgd = SGD(lr=1e-4,momentum=0.9)\n",
    "    model.compile(optimizer='adadelta',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', F1])\n",
    "\n",
    "    # early_stop = EarlyStopping(monitor='val_acc', patience=10, verbose=0, mode='auto')\n",
    "    mcp_save = ModelCheckpoint(best_model_weights_path, save_best_only=True, monitor='val_F1', verbose=1, mode='max')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_F1', factor=0.9, patience=5, \\\n",
    "                                       min_lr=1e-4, verbose=1, mode='max') # change factor, patience and min_lr\n",
    "\n",
    "    history = model.fit(X_train, y_train_cat, batch_size=nb_train_samples, epochs=100, validation_split=0.2, \\\n",
    "              callbacks = [mcp_save])\n",
    "    \n",
    "    # store history data in a dict\n",
    "    hist_info[key] = history.history\n",
    "    \n",
    "    # save model for reference\n",
    "    model_json = model.to_json()\n",
    "    with open(model_save_path, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(final_model_weights_path)\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    # load best model\n",
    "    # load json and create model\n",
    "    json_file = open(model_save_path, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(best_model_weights_path)\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    # make prediction for current working model\n",
    "    y_pred = loaded_model.predict(X_test, verbose=1)\n",
    "    # save to a dict for later use\n",
    "    y_preds[key] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save learning history info to csv\n",
    "np.save(learning_hist_path, hist_info)\n",
    "\n",
    "# plot summary of learning\n",
    "# matplotlib.style.use('default')\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 16}\n",
    "plt.rcParams['axes.grid'] = True\n",
    "matplotlib.rc('font', **font)\n",
    "# # list all data in history\n",
    "# print(hist_info.keys())\n",
    "# summarize history for training accuracy\n",
    "fig1 = plt.gcf()\n",
    "for key, value in models.iteritems():\n",
    "    hist_curr = hist_info[key]\n",
    "    plt.plot(hist_curr['acc'], label=key)\n",
    "#     plt.plot(hist_curr['val_acc'], label=\"test\")\n",
    "plt.title('Training accuracy, task 5')\n",
    "plt.ylim(0.2, 0.9)\n",
    "plt.xlim(0, 100)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(bbox_to_anchor=(1.04, 0.5), loc='center left', prop={'size': 10}, ncol = 1)\n",
    "plt.grid(color=(0.88,0.88,0.88), linestyle=':', linewidth=2)\n",
    "plt.show()\n",
    "fig1.savefig(acc_plt_path1, bbox_inches='tight', dpi=100)\n",
    "# summarize history for validation accuracy\n",
    "fig2 = plt.gcf()\n",
    "for key, value in models.iteritems():\n",
    "    hist_curr = hist_info[key]\n",
    "    plt.plot(hist_curr['val_acc'], label=key)\n",
    "plt.title('Validation accuracy, task 5')\n",
    "plt.ylim(0.2, 0.9)\n",
    "plt.xlim(0, 100)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(bbox_to_anchor=(1.04, 0.5), loc='center left', prop={'size': 10}, ncol = 1)\n",
    "plt.grid(color=(0.88,0.88,0.88), linestyle=':', linewidth=2)\n",
    "plt.show()\n",
    "fig2.savefig(acc_plt_path2, bbox_inches='tight', dpi=100)\n",
    "# summarize history for trainging loss\n",
    "fig3 = plt.gcf()\n",
    "for key, value in models.iteritems():\n",
    "    hist_curr = hist_info[key]\n",
    "    plt.plot(hist_curr['loss'], label=key)\n",
    "plt.title('Training loss, task 5')\n",
    "plt.xlim(0, 100)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(bbox_to_anchor=(1.04, 0.5), loc='center left', prop={'size': 10}, ncol = 1)\n",
    "plt.grid(color=(0.88,0.88,0.88), linestyle=':', linewidth=2)\n",
    "plt.show()\n",
    "fig3.savefig(loss_plt_path1, bbox_inches='tight', dpi=100)\n",
    "# summarize history for validation loss\n",
    "fig4 = plt.gcf()\n",
    "for key, value in models.iteritems():\n",
    "    hist_curr = hist_info[key]\n",
    "    plt.plot(hist_curr['val_loss'], label=key)\n",
    "plt.title('Validation loss, task 5')\n",
    "plt.xlim(0, 100)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(bbox_to_anchor=(1.04, 0.5), loc='center left', prop={'size': 10}, ncol = 1)\n",
    "plt.grid(color=(0.88,0.88,0.88), linestyle=':', linewidth=2)\n",
    "plt.show()\n",
    "fig4.savefig(loss_plt_path2, bbox_inches='tight', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print y_preds\n",
    "print len(y_preds)\n",
    "# print y_preds[next(iter(y_preds))]\n",
    "k = y_preds[next(iter(y_preds))]\n",
    "y_pred = np.zeros((nb_test_samples, num_classes))\n",
    "# print y_pred\n",
    "# y_pred = y_pred + y_preds[next(iter(y_preds))]\n",
    "# print y_pred\n",
    "for key, value in models.iteritems():\n",
    "    curr_pred = y_preds[key]\n",
    "    y_pred = y_pred + curr_pred / len(y_preds)\n",
    "# print y_pred\n",
    "# print np.sum(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction and save to csv\n",
    "Y_predict = y_pred.argmax(axis=1)\n",
    "Y_predict = Y_predict[:,np.newaxis]\n",
    "# print(Y_predict)\n",
    "print(Y_predict.shape)\n",
    "index = np.arange(0, nb_test_samples)\n",
    "out = np.concatenate((index[:, np.newaxis], Y_predict), axis = 1)\n",
    "with open(Y_predict_path, \"wb\") as f:\n",
    "    f.write(b'Index,Pred\\n')\n",
    "    np.savetxt(f, out.astype(int), fmt='%i', delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
